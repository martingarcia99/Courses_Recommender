{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/meteng/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/meteng/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "DATABASE_NAME = os.getenv(\"DATABASE_NAME\")\n",
    "DATABASE_PASSWORD = os.getenv(\"DATABASE_PASSWORD\")\n",
    "\n",
    "client = MongoClient(\"mongodb+srv://martin:{}@cluster0.ehkpp.mongodb.net/{}?retryWrites=true&w=majority\".format(DATABASE_PASSWORD,DATABASE_NAME))\n",
    "\n",
    "#client = MongoClient()\n",
    "db = client.get_database(DATABASE_NAME)\n",
    "\n",
    "lectures = db.lecture_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "name\ndescription\nlearning_targets\npre_qualifications\nlecture_id\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                name  \\\n",
       "0                                 Learning Analytics   \n",
       "1                                Cloud, Web & Mobile   \n",
       "2                                Distributed Systems   \n",
       "3  Natural-Language-based Human-Computer Interaction   \n",
       "4                          Advanced Web Technologies   \n",
       "\n",
       "                                         description  \\\n",
       "0  [learning, analytics, la, attracted, great, de...   \n",
       "1  [lecture, present, theoretical, practical, asp...   \n",
       "2  [lecture, present, important, concept, protoco...   \n",
       "3  [content, detail, level, language, analysis, l...   \n",
       "4  [world, wide, web, underlying, technology, inc...   \n",
       "\n",
       "                                    learning_targets  \\\n",
       "0  [successfully, completing, module, student, kn...   \n",
       "1  [student, know, understand, architecture, algo...   \n",
       "2  [student, know, principle, protocol, algorithm...   \n",
       "3  [student, learn, problem, occur, naturallangua...   \n",
       "4  [successfully, completing, module, student, kn...   \n",
       "\n",
       "                               pre_qualifications  lecture_id  \n",
       "0                                              []           2  \n",
       "1                                              []           3  \n",
       "2                                              []           4  \n",
       "3                                              []           5  \n",
       "4  [basic, knowledge, web, technology, requiered]           6  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>description</th>\n      <th>learning_targets</th>\n      <th>pre_qualifications</th>\n      <th>lecture_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Learning Analytics</td>\n      <td>[learning, analytics, la, attracted, great, de...</td>\n      <td>[successfully, completing, module, student, kn...</td>\n      <td>[]</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cloud, Web &amp; Mobile</td>\n      <td>[lecture, present, theoretical, practical, asp...</td>\n      <td>[student, know, understand, architecture, algo...</td>\n      <td>[]</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Distributed Systems</td>\n      <td>[lecture, present, important, concept, protoco...</td>\n      <td>[student, know, principle, protocol, algorithm...</td>\n      <td>[]</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Natural-Language-based Human-Computer Interaction</td>\n      <td>[content, detail, level, language, analysis, l...</td>\n      <td>[student, learn, problem, occur, naturallangua...</td>\n      <td>[]</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Advanced Web Technologies</td>\n      <td>[world, wide, web, underlying, technology, inc...</td>\n      <td>[successfully, completing, module, student, kn...</td>\n      <td>[basic, knowledge, web, technology, requiered]</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "ps = PorterStemmer() \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "stop_words = set(stopwords.words('english')) \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df = pd.DataFrame(list(lectures.find()))\n",
    "data=df.drop(columns = ['_id', 'avg_rating', 'assigned_people', 'language', 'course_format', 'study_courses', 'positive_comments', 'negative_comments', 'semester', 'comments'])\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    html_free = soup.get_text()\n",
    "    \n",
    "    return html_free\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct = ''.join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "\n",
    "def remove_stop_word(text):\n",
    "\n",
    "    no_stop_word = [c for c in text if c not in stop_words]\n",
    "    \n",
    "    return no_stop_word\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    wordnet_tagged = map(lambda x: (x[0], get_wordnet_pos(x[1])), text)\n",
    "    lemmatized_sentence = []\n",
    "        \n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def word_lemmatizer2(text):\n",
    "    lem_text = [ lemmatizer.lemmatize(i[0]) for i in text]\n",
    "    return lem_text\n",
    "    \n",
    "def word_stemmer(text):\n",
    "    stem_text = [ps.stem(i[0]) for i in text]\n",
    "    return stem_text\n",
    "\n",
    "def clean_data(data):\n",
    "    html_free_data = data.apply(lambda x: remove_html(x))\n",
    "    no_punct_data = html_free_data.apply(lambda x: remove_punctuation(x))\n",
    "    tokenized_data = no_punct_data.apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "    no_stop_word_data = tokenized_data.apply(lambda x: remove_stop_word(x))         \n",
    "    tagged_data = no_stop_word_data.apply(lambda x: nltk.pos_tag(x))\n",
    "    lemmatized_data = tagged_data.apply(lambda x: word_lemmatizer2(x))\n",
    "\n",
    "    return lemmatized_data\n",
    "\n",
    "for col in data.columns:\n",
    "    print(col)\n",
    "\n",
    "data['description'] = clean_data(data['description'])\n",
    "data['learning_targets'] = clean_data(data['learning_targets'])\n",
    "data['pre_qualifications'] = clean_data(data['pre_qualifications'])\n",
    "\n",
    "#data['description'] = data['description'].apply(lambda x: word_stemmer(x))\n",
    "\"\"\"\n",
    "for (columnName, columnData) in data['pre_qualifications'].iteritems():\n",
    "   print('Colunm Name : ', columnName)\n",
    "   print('Column Contents : ', columnData)\n",
    "\"\"\"\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"   \n",
    "input_text = 'I am passing the input sentence here. so we will see what happens with this and.'\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(input_text) \n",
    "\n",
    "#print(\"studies:\", lemmatizer.lemmatize(\"studies\")) \n",
    "#print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "#print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"n\"))\n",
    "\n",
    "def clean_text(text):#will replace the html characters with \" \"\n",
    "    text=re.sub('<.*?>', ' ', text)  \n",
    "    #To remove the punctuations\n",
    "    text = text.translate(str.maketrans(' ',' ',string.punctuation))\n",
    "    #will consider only alphabets and numerics\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)  \n",
    "    #will replace newline with space\n",
    "    text = re.sub(\"\\n\",\" \",text)\n",
    "    #will convert to lower case\n",
    "    text = text.lower()\n",
    "    # will split and join the words\n",
    "    text=' '.join(text.split())\n",
    "    return text#Running the Funtion\n",
    "\n",
    "new_text=clean_text(input_text)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python38364bitbasecondac005070830a3407b879e76c9e0c60848"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}